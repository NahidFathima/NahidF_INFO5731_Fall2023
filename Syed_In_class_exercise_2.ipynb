{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NahidFathima/NahidF_INFO5731_Fall2023/blob/main/Syed_In_class_exercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW5_oFVd9-pY"
      },
      "source": [
        "## The second In-class-exercise (09/13/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAzh1U0sE5I5"
      },
      "source": [
        "Kindly use the provided .ipynb document to write your code or respond to the questions. Avoid generating a new file.\n",
        "Execute all the cells before your final submission."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpgvZQdRE-HV"
      },
      "source": [
        "This in-class exercise is due tomorrow September 14, 2023 at 11:59 PM. No late submissions will be considered."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QBZI-je9-pZ"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWoKpYQT9-pa"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LmNR3kw9-pa"
      },
      "source": [
        "**Question:**\n",
        "How do pricing strategies impact the sales performance of handmade jewelry on Etsy? Specifically, does dynamic pricing based on factors like seasonality, competition, and materials used lead to higher sales and profitability for Etsy sellers?\n",
        "\n",
        "**Data Collection:**\n",
        "Data needs to be collected from Etsy:\n",
        "\n",
        "- Product name\n",
        "- Product link\n",
        "- Price\n",
        "- Ratings\n",
        "\n",
        "The data will be collected by scraping Etsy listings for handmade jewelry earrings. This involves making web requests, parsing the HTML, and extracting the relevant information.\n",
        "\n",
        "Web scraping tools like Beautiful Soup and requests are used to extract data from the Etsy search results page. The code navigates through the HTML structure, targeting elements with specific class names for product name, link, price, and ratings. The collected data can be saved in a CSV file for further analysis.\n",
        "\n",
        "This data will serve as the foundation for understanding how pricing strategies influence the sales performance of handmade jewelry on Etsy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlxTLRNm9-pa"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "E5cSoIGUkO1n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "457125a8-bdaf-4a6c-f9a9-efef4d8c9b79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to etsy_handmade_jewelry_sampledata.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "# URL of the Etsy search results page\n",
        "base_url = \"https://www.etsy.com/search?q=handmade+jewelry+earrings+dangle&ref=s2qit&explicit=1&s2qii=4&s2qit=as&prq=handmade+jewelry\"\n",
        "\n",
        "# Function to scrape product data\n",
        "def scrape_etsy_products(base_url, num_samples=1000):\n",
        "    products = []\n",
        "    page_number = 1\n",
        "\n",
        "    while len(products) < num_samples:\n",
        "        # Make a GET request to the Etsy search results page\n",
        "        url = f\"{base_url}&page={page_number}\"\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find product listings on the page\n",
        "        listings = soup.find_all('div', class_='v2-listing-card__info')\n",
        "\n",
        "        if not listings:\n",
        "            break\n",
        "\n",
        "        for listing in listings:\n",
        "            product = {}\n",
        "\n",
        "            # Extract product name\n",
        "            product_name_elem = listing.find('h3', class_='wt-text-caption v2-listing-card__title wt-text-truncate')\n",
        "            if product_name_elem:\n",
        "                product['name'] = product_name_elem.text.strip()\n",
        "            else:\n",
        "                product['name'] = 'Name not available'\n",
        "\n",
        "            # Extract product link\n",
        "            product_link_elem = listing.find('a', class_='listing-link wt-display-inline-block be92086f5a29fa263  logged')\n",
        "            if product_link_elem and 'href' in product_link_elem.attrs:\n",
        "                product['link'] = \"https://www.etsy.com\" + product_link_elem['href']\n",
        "            else:\n",
        "                product['link'] = 'Link not available'\n",
        "\n",
        "            # Extract product price\n",
        "            price_span = listing.find('span', class_='currency-value')\n",
        "            if price_span:\n",
        "                product['price'] = price_span.text.strip()\n",
        "            else:\n",
        "                product['price'] = 'Price not available'\n",
        "\n",
        "            # Extract product ratings if available\n",
        "            ratings_div = listing.find('div', class_='wt-align-items-center wt-max-height-full wt-display-flex-xs flex-direction-row-xs wt-text-title-small wt-no-wrap')\n",
        "            if ratings_div:\n",
        "                product['ratings'] = ratings_div.text.strip()\n",
        "            else:\n",
        "                product['ratings'] = 'No ratings'\n",
        "\n",
        "            products.append(product)\n",
        "\n",
        "            if len(products) >= num_samples:\n",
        "                break\n",
        "\n",
        "        page_number += 1\n",
        "\n",
        "        # Sleep briefly to avoid overloading the server\n",
        "        time.sleep(1)\n",
        "\n",
        "    return products\n",
        "\n",
        "# Collect 1000 data samples\n",
        "data_samples = scrape_etsy_products(base_url, num_samples=1000)\n",
        "\n",
        "# Save data to a CSV file\n",
        "csv_filename = \"etsy_handmade_jewelry_sampledata.csv\"\n",
        "with open(csv_filename, mode='w', newline='', encoding='utf-8') as csv_file:\n",
        "    fieldnames = ['name', 'link', 'price', 'ratings']\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writeheader()\n",
        "    for product in data_samples:\n",
        "        writer.writerow(product)\n",
        "\n",
        "print(f\"Data saved to {csv_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px6wgvog9-pa"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"information retrieval\". The articles should be published in the last 10 years (2013-2023).\n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5rjlclf9-pb"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "# Function to scrape data from an article page\n",
        "def scrape_article(article_url):\n",
        "    response = requests.get(article_url)\n",
        "    # Check if the request was successful\n",
        "    if response.status_code != 200:\n",
        "        return None\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extract and format the article data\n",
        "    return {\n",
        "        'Title': soup.find(\"h1\", class_=\"citation__title\").text.strip(),\n",
        "        'Authors': [author.text.strip() for author in soup.select(\"span.loa__name\")],\n",
        "        'Year': soup.find(\"span\", class_=\"epub-section__date\").text.strip(),\n",
        "        'Abstract': soup.find(\"div\", class_=\"abstract__content\").text.strip(),\n",
        "        'Venue': soup.select_one(\"a.publication-title-link\").text.strip()\n",
        "    }\n",
        "\n",
        "# Function to scrape ACM articles based on keyword and quantity\n",
        "def scrape_acm_articles(keyword, num_articles):\n",
        "    base_url = f\"https://dl.acm.org/doSearch?query={keyword}&FullText=true&startPage=0&pageSize={num_articles}\"\n",
        "    articles = []\n",
        "\n",
        "    response = requests.get(base_url)\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        # Find all article items in the search results\n",
        "        search_results = soup.find_all(\"li\", class_=\"search__item issue-item-container\")\n",
        "\n",
        "        for result in search_results:\n",
        "            article_link = result.find(\"a\", href=True)\n",
        "            if article_link:\n",
        "                article_url = \"https://dl.acm.org\" + article_link['href']\n",
        "                article_data = scrape_article(article_url)\n",
        "                if article_data:\n",
        "                    articles.append(article_data)\n",
        "\n",
        "    return articles\n",
        "\n",
        "# Main function to scrape and save data\n",
        "def main():\n",
        "    keyword = \"information retrieval\"\n",
        "    num_articles = 1000\n",
        "    articles = scrape_acm_articles(keyword, num_articles)\n",
        "\n",
        "    if articles:\n",
        "        # Filter articles published in the last 10 years (2013-2023)\n",
        "        current_year = datetime.now().year\n",
        "        filtered_articles = [article for article in articles if int(article['Year']) >= 2013 and int(article['Year']) <= current_year]\n",
        "\n",
        "        if filtered_articles:\n",
        "            # Save data to a CSV file\n",
        "            with open('acm_articles.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "                fieldnames = ['Title', 'Authors', 'Year', 'Abstract', 'Venue']\n",
        "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "                writer.writeheader()\n",
        "                for article in filtered_articles:\n",
        "                    writer.writerow(article)\n",
        "            print(f\"Scraped and saved {len(filtered_articles)} articles to 'acm_articles.csv'.\")\n",
        "        else:\n",
        "            print(\"No articles found within the specified date range.\")\n",
        "    else:\n",
        "        print(\"No articles found.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT3CNj_V9-pb"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data.\n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bd0X0EqTkO1p",
        "outputId": "6791f47f-bb12-4b20-bf4f-51843e3c30b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting instaloader\n",
            "  Downloading instaloader-4.10.tar.gz (62 kB)\n",
            "Requirement already satisfied: requests>=2.4 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from instaloader) (2.27.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from requests>=2.4->instaloader) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from requests>=2.4->instaloader) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from requests>=2.4->instaloader) (2023.7.22)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from requests>=2.4->instaloader) (1.26.9)\n",
            "Building wheels for collected packages: instaloader\n",
            "  Building wheel for instaloader (setup.py): started\n",
            "  Building wheel for instaloader (setup.py): finished with status 'done'\n",
            "  Created wheel for instaloader: filename=instaloader-4.10-py3-none-any.whl size=64299 sha256=b6183a82060974df1b22e8ffc5303407c0f64e4cff108b20812a51f99b18751e\n",
            "  Stored in directory: c:\\users\\nahid\\appdata\\local\\pip\\cache\\wheels\\30\\d3\\e9\\a15fc8e2e997e4dc75983128dc3f48a051476301f8422c8cde\n",
            "Successfully built instaloader\n",
            "Installing collected packages: instaloader\n",
            "Successfully installed instaloader-4.10\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install instaloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZRSrzYSkO1p",
        "outputId": "3fd2c259-781c-4b96-92a5-6eaa3e2a7857"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: win_unicode_console in c:\\users\\nahid\\anaconda3\\lib\\site-packages (0.5)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade win_unicode_console\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qw13FCDpkO1q",
        "outputId": "0fd77143-4c4f-48de-c1db-890c350b6ec0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV file saved to instagram_unt.csv\n"
          ]
        }
      ],
      "source": [
        "import instaloader\n",
        "import csv\n",
        "\n",
        "# Initialize Instaloader\n",
        "L = instaloader.Instaloader()\n",
        "\n",
        "# Target Instagram account\n",
        "username = \"unt\"\n",
        "\n",
        "# Load the profile of the target account\n",
        "profile = instaloader.Profile.from_username(L.context, username)\n",
        "\n",
        "# Initialize a list to store the collected data\n",
        "data = []\n",
        "\n",
        "# Collect up to 1000 posts\n",
        "for post in profile.get_posts():\n",
        "    # Extract relevant data like Get username, post_time, and post_text\n",
        "    title = username\n",
        "    year = post.date.year\n",
        "    authors = username\n",
        "    abstract = post.caption if post.caption else \"\"\n",
        "\n",
        "    # Append data to the list\n",
        "    data.append([title, year, authors, abstract])\n",
        "\n",
        "    # Break the loop once 1000 posts are collected\n",
        "    if len(data) >= 1000:\n",
        "        break\n",
        "\n",
        "# Save the collected data to a CSV file\n",
        "csv_file_name = f'instagram_{username}.csv'\n",
        "with open(csv_file_name, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "    csv_writer.writerow(['Title', 'Year', 'Authors', 'Abstract'])\n",
        "    csv_writer.writerows(data)\n",
        "\n",
        "print(f\"CSV file saved to {csv_file_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4sEtTVykO1q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}