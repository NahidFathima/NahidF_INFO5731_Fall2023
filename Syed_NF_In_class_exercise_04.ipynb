{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NahidFathima/NahidF_INFO5731_Fall2023/blob/main/Syed_NF_In_class_exercise_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnjnJ1w802Iu"
      },
      "source": [
        "# **The fourth in-class-exercise (40 points in total, 03/28/2022)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3w3uYYb02Iv"
      },
      "source": [
        "Question description: Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lph_yWrk02Iw"
      },
      "source": [
        "## (1) (10 points) Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJaWgfvZ02Iw",
        "outputId": "3f988bcd-8a63-4ca8-aa96-1373c3327769"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.012*\"one\" + 0.010*\"say\" + 0.009*\"drag\" + 0.008*\"also\" + 0.006*\"first\"'),\n",
              " (1,\n",
              "  '0.008*\"also\" + 0.007*\"say\" + 0.007*\"one\" + 0.005*\"indigenous\" + 0.005*\"price\"'),\n",
              " (2,\n",
              "  '0.014*\"price\" + 0.012*\"company\" + 0.011*\"streaming\" + 0.009*\"service\" + 0.008*\"business\"'),\n",
              " (3,\n",
              "  '0.010*\"also\" + 0.009*\"work\" + 0.009*\"like\" + 0.008*\"straw\" + 0.008*\"plastic\"'),\n",
              " (4,\n",
              "  '0.010*\"say\" + 0.009*\"price\" + 0.007*\"company\" + 0.006*\"pricing\" + 0.006*\"straw\"'),\n",
              " (5,\n",
              "  '0.008*\"say\" + 0.007*\"one\" + 0.006*\"price\" + 0.006*\"climate\" + 0.006*\"business\"'),\n",
              " (6,\n",
              "  '0.006*\"straw\" + 0.006*\"say\" + 0.005*\"price\" + 0.005*\"also\" + 0.005*\"company\"'),\n",
              " (7,\n",
              "  '0.008*\"say\" + 0.007*\"also\" + 0.007*\"community\" + 0.006*\"climate\" + 0.005*\"text\"')]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import csv  # Importing the CSV module for reading CSV files\n",
        "import gensim  # Importing the Gensim library for topic modeling\n",
        "from gensim import corpora  # Importing the corpora module for creating a document-term matrix\n",
        "from gensim.models import CoherenceModel  # Importing CoherenceModel for computing coherence score\n",
        "from nltk.tokenize import word_tokenize  # Importing the word_tokenize tool for tokenization\n",
        "from nltk.corpus import stopwords  # Importing a list of common stop words\n",
        "from nltk.stem import WordNetLemmatizer  # Importing the WordNetLemmatizer for lemmatization\n",
        "\n",
        "# Reading the news articles from the CSV file\n",
        "news_articles = []\n",
        "with open('news_articles.csv', mode='r', encoding='utf-8') as file:  # Opening the CSV file\n",
        "    reader = csv.reader(file)  # Creating a reader object\n",
        "    for row in reader:  # Looping through each row in the CSV\n",
        "        news_articles.append(row[2])  # Appending the text from the third column to the news_articles list\n",
        "\n",
        "# Rest of the code remains the same\n",
        "\n",
        "# Getting set of common stop words in english\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Lemmatization to convert words into their base form\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Tokenizing, removing any stop words and finally lemmatize the articles\n",
        "tokenized_articles = [[lemmatizer.lemmatize(word) for word in word_tokenize(article.lower()) if word.isalpha() and word not in stop_words] for article in news_articles]\n",
        "\n",
        "# Let's create a dictionary of tokenized articles\n",
        "dictionary = corpora.Dictionary(tokenized_articles)\n",
        "\n",
        "# Create a corpus using the tokenized dictionary\n",
        "corpus = [dictionary.doc2bow(article) for article in tokenized_articles]\n",
        "\n",
        "# Function to compute coherence values for different numbers of topics\n",
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):  # Iterating over a range of topic numbers\n",
        "        model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary)  # Creating an LDA model\n",
        "        model_list.append(model)\n",
        "        coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')  # Computing coherence score\n",
        "        coherence_values.append(coherence_model.get_coherence())\n",
        "    return model_list, coherence_values\n",
        "\n",
        "# Compute coherence values for different topic numbers\n",
        "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=tokenized_articles, start=2, limit=10, step=1)\n",
        "\n",
        "# Selecting the model with the highest coherence value\n",
        "optimal_model = model_list[coherence_values.index(max(coherence_values))]\n",
        "\n",
        "# Print the topics and their respective keywords\n",
        "optimal_model.print_topics(num_words=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCihaGUH02Ix"
      },
      "source": [
        "## (2) (10 points) Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1brlkSN602Ix",
        "outputId": "9c769f17-623e-4130-e1a1-a045aec5367f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary of 10 topics generated by LSA:\n",
            "Topic 1: says, like, ad, climate, million\n",
            "Topic 2: like, sea, catalogue, brought, green\n",
            "Topic 3: pricing, ad, companies, price, prices\n",
            "Topic 4: work, 2016, museum, seen, said\n",
            "Topic 5: hours, view, difficult, sense, later\n",
            "Topic 6: ll, just, planet, places, international\n",
            "Topic 7: climate, says, change, pricing, natural\n",
            "Topic 8: like, known, came, pricing, catalogue\n",
            "Topic 9: ad, company, time, free, business\n",
            "Topic 10: adopted, announced, blue, hours, green\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "# Read news articles from the CSV file\n",
        "news_articles = []\n",
        "with open('news_articles.csv', mode='r', encoding='utf-8') as file:  # Opening the CSV file\n",
        "    reader = csv.reader(file)  # Creating a reader object\n",
        "    for row in reader:\n",
        "        news_articles.append(row[2])  # Appending the text from the third column to the news_articles list\n",
        "\n",
        "# Set the number of topics to be determined by the coherence score\n",
        "coherence_scores = []\n",
        "for num_topics in range(2, 11):  # Trying different numbers of topics\n",
        "    # Creating a TfidfVectorizer to convert the collection of raw documents to a matrix of TF-IDF features\n",
        "    vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
        "    tfidf_matrix = vectorizer.fit_transform(news_articles)  # Fitting and transforming the data\n",
        "    lsa_model = TruncatedSVD(num_topics)  # Creating an LSA model with a specific number of topics\n",
        "    lsa_pipeline = make_pipeline(lsa_model, Normalizer(copy=False))  # Creating an LSA pipeline with normalization\n",
        "    lsa_matrix = lsa_pipeline.fit_transform(tfidf_matrix)  # Fitting and transforming the data using the LSA pipeline\n",
        "    coherence_scores.append((num_topics, lsa_model.explained_variance_ratio_.sum()))  # Appending the coherence score\n",
        "\n",
        "# Choosing the number of topics that maximizes the coherence score\n",
        "optimal_num_topics = max(coherence_scores, key=lambda x: x[1])[0]\n",
        "\n",
        "# Generating topics using LSA with the optimal number of topics\n",
        "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')  # Creating a TfidfVectorizer\n",
        "tfidf_matrix = vectorizer.fit_transform(news_articles)  # Fitting and transforming the data\n",
        "lsa_model = TruncatedSVD(optimal_num_topics)  # Creating an LSA model with the optimal number of topics\n",
        "lsa_pipeline = make_pipeline(lsa_model, Normalizer(copy=False))  # Creating an LSA pipeline with normalization\n",
        "lsa_matrix = lsa_pipeline.fit_transform(tfidf_matrix)  # Fitting and transforming the data using the LSA pipeline\n",
        "\n",
        "# Getting the terms from the vectorizer\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Printing the summary of the topics\n",
        "print(f\"Summary of {optimal_num_topics} topics generated by LSA:\")\n",
        "for i, component in enumerate(lsa_model.components_):  # Iterating over the components\n",
        "    top_terms = [terms[j] for j in component.argsort()[:-6:-1]]  # Extracting the top terms for each topic\n",
        "    print(f\"Topic {i + 1}: {', '.join(top_terms)}\")  # Printing the top terms for each topic\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRlHdmIz02Ix"
      },
      "source": [
        "## (3) (10 points) Generate K topics by using  lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smOVQxnh02Iy",
        "outputId": "84846b79-3cde-49a4-cfcf-fefc68048461"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lda2vec in c:\\users\\nahid\\anaconda3\\lib\\site-packages (0.16.10)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install lda2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdIo2L8g02Iy",
        "outputId": "a200e7d7-f4db-41e1-b9aa-2f20b8fb2b9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyldavis\n",
            "  Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
            "Requirement already satisfied: setuptools in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pyldavis) (61.2.0)\n",
            "Collecting pandas>=2.0.0\n",
            "  Downloading pandas-2.1.2-cp39-cp39-win_amd64.whl (10.8 MB)\n",
            "Collecting joblib>=1.2.0\n",
            "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "Collecting numpy>=1.24.2\n",
            "  Downloading numpy-1.26.1-cp39-cp39-win_amd64.whl (15.8 MB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pyldavis) (1.1.3)\n",
            "Requirement already satisfied: gensim in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pyldavis) (4.1.2)\n",
            "Requirement already satisfied: numexpr in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pyldavis) (2.8.1)\n",
            "Collecting funcy\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pyldavis) (2.11.3)\n",
            "Requirement already satisfied: scipy in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pyldavis) (1.7.3)\n",
            "Collecting tzdata>=2022.1\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pandas>=2.0.0->pyldavis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pandas>=2.0.0->pyldavis) (2021.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyldavis) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->pyldavis) (2.2.0)\n",
            "Collecting scipy\n",
            "  Downloading scipy-1.11.3-cp39-cp39-win_amd64.whl (44.3 MB)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from gensim->pyldavis) (6.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from jinja2->pyldavis) (2.0.1)\n",
            "Requirement already satisfied: packaging in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from numexpr->pyldavis) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from packaging->numexpr->pyldavis) (3.0.4)\n",
            "Installing collected packages: numpy, tzdata, scipy, joblib, pandas, funcy, pyldavis\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.7.3\n",
            "    Uninstalling scipy-1.7.3:\n",
            "      Successfully uninstalled scipy-1.7.3\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.1.0\n",
            "    Uninstalling joblib-1.1.0:\n",
            "      Successfully uninstalled joblib-1.1.0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.4.2\n",
            "    Uninstalling pandas-1.4.2:\n",
            "      Successfully uninstalled pandas-1.4.2\n",
            "Successfully installed funcy-2.0 joblib-1.3.2 numpy-1.26.1 pandas-2.1.2 pyldavis-3.4.1 scipy-1.11.3 tzdata-2023.3\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "daal4py 2021.5.0 requires daal==2021.4.0, which is not installed.\n",
            "tensorflow-intel 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.26.1 which is incompatible.\n",
            "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.26.1 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "pip install pyldavis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKV4zmaB02Iy",
        "outputId": "1adfa4b0-553c-4f41-ebb0-dd8f62bc6dde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyLDAvis in c:\\users\\nahid\\anaconda3\\lib\\site-packages (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.24.2 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.25.2)\n",
            "Requirement already satisfied: scipy in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.11.3)\n",
            "Requirement already satisfied: pandas>=2.0.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.1.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.2.0)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pyLDAvis) (3.1.2)\n",
            "Requirement already satisfied: numexpr in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.8.7)\n",
            "Requirement already satisfied: funcy in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.3.0)\n",
            "Requirement already satisfied: gensim in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pyLDAvis) (4.3.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pyLDAvis) (68.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->pyLDAvis) (2.2.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from gensim->pyLDAvis) (5.2.1)\n",
            "Collecting FuzzyTM>=0.4.0 (from gensim->pyLDAvis)\n",
            "  Downloading FuzzyTM-2.0.5-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from jinja2->pyLDAvis) (2.1.1)\n",
            "Collecting pyfume (from FuzzyTM>=0.4.0->gensim->pyLDAvis)\n",
            "  Downloading pyFUME-0.2.25-py3-none-any.whl (67 kB)\n",
            "     ---------------------------------------- 0.0/67.1 kB ? eta -:--:--\n",
            "     ---------------------------------------- 67.1/67.1 kB 1.8 MB/s eta 0:00:00\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
            "Collecting simpful (from pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis)\n",
            "  Downloading simpful-2.11.0-py3-none-any.whl (32 kB)\n",
            "Collecting fst-pso (from pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis)\n",
            "  Downloading fst-pso-1.8.1.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting miniful (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis)\n",
            "  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Building wheels for collected packages: fst-pso, miniful\n",
            "  Building wheel for fst-pso (setup.py): started\n",
            "  Building wheel for fst-pso (setup.py): finished with status 'done'\n",
            "  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20451 sha256=82f4655aac69a41f8a27ac22a9bbebfff143ade35af4682da3acef9138b5acef\n",
            "  Stored in directory: c:\\users\\nahid\\appdata\\local\\pip\\cache\\wheels\\99\\66\\48\\d7ce0c6927f6abf167bbcdee537affc7b92c03632f78028411\n",
            "  Building wheel for miniful (setup.py): started\n",
            "  Building wheel for miniful (setup.py): finished with status 'done'\n",
            "  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3517 sha256=5011c62df5a4f898139554407b0571d88315a72101e6f3836955bcb9c6b3b4bd\n",
            "  Stored in directory: c:\\users\\nahid\\appdata\\local\\pip\\cache\\wheels\\d9\\c7\\71\\db1d4646d963b34c530667501d3d6f34c0825eaffae2f0f2cb\n",
            "Successfully built fst-pso miniful\n",
            "Installing collected packages: simpful, miniful, fst-pso, pyfume, FuzzyTM\n",
            "Successfully installed FuzzyTM-2.0.5 fst-pso-1.8.1 miniful-0.0.6 pyfume-0.2.25 simpful-2.11.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install pyLDAvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijKZqXsy02Iy",
        "outputId": "4f00eb65-7232-4034-fa66-3b903afc1c5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting preprocess\n",
            "  Downloading preprocess-2.0.0-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: future in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from preprocess) (0.18.3)\n",
            "Installing collected packages: preprocess\n",
            "Successfully installed preprocess-2.0.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOft8pR-02Iy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from lda2vec import preprocess, Corpus, Lda2Vec\n",
        "\n",
        "try:\n",
        "    # Load the data\n",
        "    data_frame = pd.read_csv('news_articles.csv')\n",
        "\n",
        "    # Preprocess the data\n",
        "    vectorizer = CountVectorizer()  # Initialize the CountVectorizer\n",
        "    X = vectorizer.fit_transform(data_frame['text'])  # Transform the text data into a document-term matrix\n",
        "    vocab = vectorizer.get_feature_names_out()  # Get the vocabulary of the text data\n",
        "\n",
        "    # Create a corpus object\n",
        "    corpus_obj = Corpus()  # Initialize the corpus object\n",
        "    corpus_obj.fit(X)  # Fit the document-term matrix to the corpus object\n",
        "\n",
        "    # Train the LDA2Vec model to find the optimal number of topics\n",
        "    lda2vec_model = Lda2Vec(num_topics=10, passes=5, vocab=vocab)  # Initialize the LDA2Vec model\n",
        "    lda2vec_model.fit(corpus_obj)  # Fit the corpus object to the LDA2Vec model\n",
        "\n",
        "    # Get the coherence scores for different numbers of topics\n",
        "    coherence_scores = []\n",
        "    for num_topics in range(5, 15):\n",
        "        model = Lda2Vec(num_topics=num_topics, passes=5, vocab=vocab)  # Initialize the LDA2Vec model with varying topics\n",
        "        model.fit(corpus_obj)  # Fit the corpus object to the LDA2Vec model\n",
        "        coherence_scores.append((num_topics, model.get_coherence()))  # Append the coherence score for each number of topics\n",
        "\n",
        "    # Find the number of topics with the highest coherence score\n",
        "    best_num_topics = max(coherence_scores, key=lambda x: x[1])[0]  # Find the number of topics with the highest coherence score\n",
        "\n",
        "    # Train the LDA2Vec model with the best number of topics\n",
        "    final_model = Lda2Vec(num_topics=best_num_topics, passes=5, vocab=vocab)  # Initialize the LDA2Vec model with the best number of topics\n",
        "    final_model.fit(corpus_obj)  # Fit the corpus object to the final LDA2Vec model\n",
        "\n",
        "    # Get the topics\n",
        "    topics = final_model.print_topics(num_words=5)  # Get the topics with the specified number of words\n",
        "\n",
        "    # Display the topics\n",
        "    for topic in topics:\n",
        "        print(\"Topic {}: {}\".format(topic[0], topic[1]))  # Print the topics with their corresponding words\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSZbx34i02Iz"
      },
      "source": [
        "## (4) (10 points) Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGkU9Vfn02Iz",
        "outputId": "2848cb35-fee0-4964-cda0-0ad7414e3a26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bertopic\n",
            "  Downloading bertopic-0.15.0-py2.py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from bertopic) (1.26.1)\n",
            "Collecting hdbscan>=0.8.29 (from bertopic)\n",
            "  Downloading hdbscan-0.8.33.tar.gz (5.2 MB)\n",
            "     ---------------------------------------- 0.0/5.2 MB ? eta -:--:--\n",
            "      --------------------------------------- 0.1/5.2 MB 1.7 MB/s eta 0:00:03\n",
            "     - -------------------------------------- 0.2/5.2 MB 2.3 MB/s eta 0:00:03\n",
            "     --- ------------------------------------ 0.5/5.2 MB 4.0 MB/s eta 0:00:02\n",
            "     ------- -------------------------------- 1.0/5.2 MB 5.2 MB/s eta 0:00:01\n",
            "     ---------- ----------------------------- 1.4/5.2 MB 5.9 MB/s eta 0:00:01\n",
            "     ------------------ --------------------- 2.4/5.2 MB 8.7 MB/s eta 0:00:01\n",
            "     ----------------------- ---------------- 3.1/5.2 MB 9.3 MB/s eta 0:00:01\n",
            "     ------------------------ --------------- 3.1/5.2 MB 9.6 MB/s eta 0:00:01\n",
            "     ------------------------ --------------- 3.1/5.2 MB 9.6 MB/s eta 0:00:01\n",
            "     ------------------------ --------------- 3.1/5.2 MB 9.6 MB/s eta 0:00:01\n",
            "     -------------------------- ------------- 3.5/5.2 MB 6.7 MB/s eta 0:00:01\n",
            "     -------------------------------- ------- 4.2/5.2 MB 7.6 MB/s eta 0:00:01\n",
            "     ---------------------------------- ----- 4.5/5.2 MB 7.4 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 5.2/5.2 MB 7.9 MB/s eta 0:00:00\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting umap-learn>=0.5.0 (from bertopic)\n",
            "  Downloading umap-learn-0.5.4.tar.gz (90 kB)\n",
            "     ---------------------------------------- 0.0/90.8 kB ? eta -:--:--\n",
            "     ---------------------------------------- 90.8/90.8 kB 5.0 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: pandas>=1.1.5 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from bertopic) (2.1.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from bertopic) (1.3.0)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from bertopic) (4.65.0)\n",
            "Collecting sentence-transformers>=0.4.1 (from bertopic)\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "     ---------------------------------------- 0.0/86.0 kB ? eta -:--:--\n",
            "     ---------------------------------------- 86.0/86.0 kB ? eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: plotly>=4.7.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from bertopic) (5.9.0)\n",
            "Collecting cython<3,>=0.27 (from hdbscan>=0.8.29->bertopic)\n",
            "  Using cached Cython-0.29.36-py2.py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: scipy>=1.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2023.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from plotly>=4.7.0->bertopic) (8.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2.post1->bertopic) (2.2.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (4.34.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (1.9.0+cpu)\n",
            "Requirement already satisfied: torchvision in c:\\users\\nahid\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.10.0+cpu)\n",
            "Requirement already satisfied: nltk in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading sentencepiece-0.1.99-cp39-cp39-win_amd64.whl (977 kB)\n",
            "     ---------------------------------------- 0.0/977.6 kB ? eta -:--:--\n",
            "     ---------- --------------------------- 266.2/977.6 kB 5.4 MB/s eta 0:00:01\n",
            "     --------------------- ---------------- 563.2/977.6 kB 5.9 MB/s eta 0:00:01\n",
            "     -------------------------------------  972.8/977.6 kB 6.8 MB/s eta 0:00:01\n",
            "     -------------------------------------- 977.6/977.6 kB 6.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.17.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from tqdm>=4.41.1->bertopic) (0.4.6)\n",
            "Requirement already satisfied: numba>=0.51.2 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from umap-learn>=0.5.0->bertopic) (0.58.0)\n",
            "Collecting pynndescent>=0.5 (from umap-learn>=0.5.0->bertopic)\n",
            "  Downloading pynndescent-0.5.10.tar.gz (1.1 MB)\n",
            "     ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
            "     ------ --------------------------------- 0.2/1.1 MB 3.5 MB/s eta 0:00:01\n",
            "     ----------- ---------------------------- 0.3/1.1 MB 4.2 MB/s eta 0:00:01\n",
            "     ---------------------- ----------------- 0.6/1.1 MB 4.5 MB/s eta 0:00:01\n",
            "     -------------------------------------- - 1.1/1.1 MB 6.3 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 1.1/1.1 MB 6.0 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: filelock in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.9.0)\n",
            "Requirement already satisfied: fsspec in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2023.9.2)\n",
            "Requirement already satisfied: requests in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (23.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.41.0)\n",
            "Collecting numpy>=1.20.0 (from bertopic)\n",
            "  Downloading numpy-1.25.2-cp39-cp39-win_amd64.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.16.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2023.10.3)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.4.0)\n",
            "Requirement already satisfied: click in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (8.1.7)\n",
            "Requirement already satisfied: pillow>=5.3.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (10.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2023.7.22)\n",
            "Downloading bertopic-0.15.0-py2.py3-none-any.whl (143 kB)\n",
            "   ---------------------------------------- 0.0/143.4 kB ? eta -:--:--\n",
            "   ---------------------------------------- 143.4/143.4 kB 8.9 MB/s eta 0:00:00\n",
            "Using cached Cython-0.29.36-py2.py3-none-any.whl (988 kB)\n",
            "Downloading numpy-1.25.2-cp39-cp39-win_amd64.whl (15.6 MB)\n",
            "   ---------------------------------------- 0.0/15.6 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.2/15.6 MB 4.1 MB/s eta 0:00:04\n",
            "    --------------------------------------- 0.3/15.6 MB 4.1 MB/s eta 0:00:04\n",
            "   - -------------------------------------- 0.5/15.6 MB 3.7 MB/s eta 0:00:05\n",
            "   - -------------------------------------- 0.6/15.6 MB 3.7 MB/s eta 0:00:05\n",
            "   -- ------------------------------------- 0.8/15.6 MB 3.4 MB/s eta 0:00:05\n",
            "   -- ------------------------------------- 1.0/15.6 MB 3.4 MB/s eta 0:00:05\n",
            "   --- ------------------------------------ 1.2/15.6 MB 3.9 MB/s eta 0:00:04\n",
            "   --- ------------------------------------ 1.5/15.6 MB 4.0 MB/s eta 0:00:04\n",
            "   ---- ----------------------------------- 1.7/15.6 MB 4.0 MB/s eta 0:00:04\n",
            "   ---- ----------------------------------- 1.9/15.6 MB 4.2 MB/s eta 0:00:04\n",
            "   ----- ---------------------------------- 2.1/15.6 MB 4.0 MB/s eta 0:00:04\n",
            "   ----- ---------------------------------- 2.2/15.6 MB 3.8 MB/s eta 0:00:04\n",
            "   ----- ---------------------------------- 2.3/15.6 MB 3.8 MB/s eta 0:00:04\n",
            "   ------ --------------------------------- 2.4/15.6 MB 3.8 MB/s eta 0:00:04\n",
            "   ------ --------------------------------- 2.5/15.6 MB 3.7 MB/s eta 0:00:04\n",
            "   ------ --------------------------------- 2.7/15.6 MB 3.6 MB/s eta 0:00:04\n",
            "   ------- -------------------------------- 2.9/15.6 MB 3.6 MB/s eta 0:00:04\n",
            "   ------- -------------------------------- 3.1/15.6 MB 3.7 MB/s eta 0:00:04\n",
            "   -------- ------------------------------- 3.2/15.6 MB 3.7 MB/s eta 0:00:04\n",
            "   -------- ------------------------------- 3.4/15.6 MB 3.6 MB/s eta 0:00:04\n",
            "   --------- ------------------------------ 3.6/15.6 MB 3.7 MB/s eta 0:00:04\n",
            "   --------- ------------------------------ 3.8/15.6 MB 3.7 MB/s eta 0:00:04\n",
            "   --------- ------------------------------ 3.9/15.6 MB 3.6 MB/s eta 0:00:04\n",
            "   ---------- ----------------------------- 4.0/15.6 MB 3.6 MB/s eta 0:00:04\n",
            "   ---------- ----------------------------- 4.0/15.6 MB 3.5 MB/s eta 0:00:04\n",
            "   ---------- ----------------------------- 4.2/15.6 MB 3.5 MB/s eta 0:00:04\n",
            "   ----------- ---------------------------- 4.3/15.6 MB 3.5 MB/s eta 0:00:04\n",
            "   ----------- ---------------------------- 4.4/15.6 MB 3.4 MB/s eta 0:00:04\n",
            "   ----------- ---------------------------- 4.6/15.6 MB 3.4 MB/s eta 0:00:04\n",
            "   ------------ --------------------------- 4.8/15.6 MB 3.4 MB/s eta 0:00:04\n",
            "   ------------ --------------------------- 5.0/15.6 MB 3.5 MB/s eta 0:00:04\n",
            "   ------------- -------------------------- 5.3/15.6 MB 3.6 MB/s eta 0:00:03\n",
            "   -------------- ------------------------- 5.7/15.6 MB 3.7 MB/s eta 0:00:03\n",
            "   -------------- ------------------------- 5.7/15.6 MB 3.7 MB/s eta 0:00:03\n",
            "   --------------- ------------------------ 5.9/15.6 MB 3.6 MB/s eta 0:00:03\n",
            "   --------------- ------------------------ 6.0/15.6 MB 3.6 MB/s eta 0:00:03\n",
            "   --------------- ------------------------ 6.2/15.6 MB 3.6 MB/s eta 0:00:03\n",
            "   ---------------- ----------------------- 6.5/15.6 MB 3.7 MB/s eta 0:00:03\n",
            "   ----------------- ---------------------- 6.7/15.6 MB 3.7 MB/s eta 0:00:03\n",
            "   ----------------- ---------------------- 7.0/15.6 MB 3.7 MB/s eta 0:00:03\n",
            "   ------------------ --------------------- 7.3/15.6 MB 3.8 MB/s eta 0:00:03\n",
            "   ------------------- -------------------- 7.8/15.6 MB 4.0 MB/s eta 0:00:02\n",
            "   -------------------- ------------------- 8.1/15.6 MB 4.0 MB/s eta 0:00:02\n",
            "   --------------------- ------------------ 8.4/15.6 MB 4.1 MB/s eta 0:00:02\n",
            "   ---------------------- ----------------- 8.9/15.6 MB 4.2 MB/s eta 0:00:02\n",
            "   ----------------------- ---------------- 9.3/15.6 MB 4.4 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 10.2/15.6 MB 4.6 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 10.5/15.6 MB 4.7 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 10.9/15.6 MB 4.9 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 11.3/15.6 MB 5.0 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 11.6/15.6 MB 5.1 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 12.2/15.6 MB 5.3 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 12.8/15.6 MB 5.8 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 13.2/15.6 MB 6.0 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 13.6/15.6 MB 6.2 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 13.8/15.6 MB 6.2 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 14.1/15.6 MB 6.4 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 14.7/15.6 MB 7.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------  15.3/15.6 MB 7.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------  15.6/15.6 MB 7.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 15.6/15.6 MB 7.7 MB/s eta 0:00:00\n",
            "Building wheels for collected packages: hdbscan, sentence-transformers, umap-learn, pynndescent\n",
            "  Building wheel for hdbscan (pyproject.toml): started\n",
            "  Building wheel for hdbscan (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.33-cp39-cp39-win_amd64.whl size=610503 sha256=e0ef2d223036008db7dadc15d478dfb4584763815457f5abc9e4678b831df5fe\n",
            "  Stored in directory: c:\\users\\nahid\\appdata\\local\\pip\\cache\\wheels\\28\\5e\\ed\\5989da4cc423a222a47cbb4fde5d6c0eff4590d922e45f233c\n",
            "  Building wheel for sentence-transformers (setup.py): started\n",
            "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125953 sha256=97611ab37703fe27201935ccf0042250eaa9b36e9ab8c3bceae88f6206005589\n",
            "  Stored in directory: c:\\users\\nahid\\appdata\\local\\pip\\cache\\wheels\\71\\67\\06\\162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
            "  Building wheel for umap-learn (setup.py): started\n",
            "  Building wheel for umap-learn (setup.py): finished with status 'done'\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.4-py3-none-any.whl size=86856 sha256=3324f66861625886a57156f876ad17ac431e95b13b008c578e137f32fbe486c3\n",
            "  Stored in directory: c:\\users\\nahid\\appdata\\local\\pip\\cache\\wheels\\e1\\8b\\ec\\51afd5b0c041b6a7dd5777ceb58cc0d645ba9454cc5a923e96\n",
            "  Building wheel for pynndescent (setup.py): started\n",
            "  Building wheel for pynndescent (setup.py): finished with status 'done'\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.10-py3-none-any.whl size=55671 sha256=11207ccfe30e5838fd74d5e7c934de9c969e03bfcf984bbfe13b36e169566811\n",
            "  Stored in directory: c:\\users\\nahid\\appdata\\local\\pip\\cache\\wheels\\12\\f9\\4d\\ec5ad1c823c710fcc4473669fdcffc8891f4bc398c841af22e\n",
            "Successfully built hdbscan sentence-transformers umap-learn pynndescent\n",
            "Installing collected packages: sentencepiece, numpy, cython, pynndescent, hdbscan, umap-learn, sentence-transformers, bertopic\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.1\n",
            "    Uninstalling numpy-1.26.1:\n",
            "      Successfully uninstalled numpy-1.26.1\n",
            "  Attempting uninstall: cython\n",
            "    Found existing installation: Cython 3.0.0\n",
            "    Uninstalling Cython-3.0.0:\n",
            "      Successfully uninstalled Cython-3.0.0\n",
            "Successfully installed bertopic-0.15.0 cython-0.29.36 hdbscan-0.8.33 numpy-1.25.2 pynndescent-0.5.10 sentence-transformers-2.2.2 sentencepiece-0.1.99 umap-learn-0.5.4\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Nahid\\anaconda3\\Lib\\site-packages\\~umpy.libs'.\n",
            "  You can safely remove it manually.\n",
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Nahid\\anaconda3\\Lib\\site-packages\\~umpy'.\n",
            "  You can safely remove it manually.\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
            "tables 3.8.0 requires blosc2~=2.0.0, which is not installed.\n",
            "featurewiz 0.2.2 requires pyarrow~=7.0.0, but you have pyarrow 11.0.0 which is incompatible.\n",
            "tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorflow 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorflow-intel 2.12.0 requires keras<2.13,>=2.12.0, but you have keras 2.10.0 which is incompatible.\n",
            "tensorflow-intel 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.25.2 which is incompatible.\n",
            "tensorflow-intel 2.12.0 requires tensorboard<2.13,>=2.12, but you have tensorboard 2.10.0 which is incompatible.\n",
            "tensorflow-intel 2.12.0 requires tensorflow-estimator<2.13,>=2.12.0, but you have tensorflow-estimator 2.10.0 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "pip install bertopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdpJIzE502Iz",
        "outputId": "d7b8f545-7f8d-47e8-817d-1b387dcf9dc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in c:\\users\\nahid\\anaconda3\\lib\\site-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.34.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.9.0+cpu)\n",
            "Requirement already satisfied: torchvision in c:\\users\\nahid\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers) (0.10.0+cpu)\n",
            "Requirement already satisfied: numpy in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: scipy in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.11.3)\n",
            "Requirement already satisfied: nltk in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.17.3)\n",
            "Requirement already satisfied: filelock in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n",
            "Requirement already satisfied: fsspec in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.9.2)\n",
            "Requirement already satisfied: requests in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.10.3)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.0)\n",
            "Requirement already satisfied: click in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
            "Requirement already satisfied: pillow>=5.3.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers) (10.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PBiwPC202Iz",
        "outputId": "fd242d47-5fbe-4f04-f336-e64152484ecf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in c:\\users\\nahid\\anaconda3\\lib\\site-packages (1.9.0+cpu)Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from torch) (4.7.1)\n"
          ]
        }
      ],
      "source": [
        "pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQaS_RxS02Iz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from bertopic import BERTopic  # Import the BERTopic library\n",
        "from sentence_transformers import SentenceTransformer  # Import the SentenceTransformer for BERT embeddings\n",
        "\n",
        "# Load the CSV file\n",
        "data = pd.read_csv('news_articles.csv')  # Load the news_articles.csv file\n",
        "\n",
        "# Extract the text data from the CSV\n",
        "articles_text = data['Text'].tolist()  # Extract the text data from the CSV\n",
        "\n",
        "# Define the sentence-transformer model for BERT embeddings\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')  # Load the pre-trained BERT-based model\n",
        "\n",
        "# Create the BERTopic model and fit it to the data\n",
        "topic_model = BERTopic(language='english', calculate_probabilities=True, embedding_model=model)  # Create a BERTopic model\n",
        "topics, _ = topic_model.fit_transform(articles_text)  # Fit the model to the text data\n",
        "\n",
        "# Get the topics with their top words\n",
        "top_topics = topic_model.get_topic_freq()  # Get the frequencies of the topics\n",
        "\n",
        "# Print the topics and their top words\n",
        "for topic_id, freq in top_topics:\n",
        "    words = topic_model.get_topic(topic_id)  # Get the top words for the topic\n",
        "    print(f\"Topic {topic_id}: {', '.join(words)}\")  # Print the topic ID and its top words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiFsZkLl02I0"
      },
      "source": [
        "## (5) (10 extra points) Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxPxHJ6B02I0"
      },
      "source": [
        "LDA (Latent Dirichlet Allocation) is adept at producing easily interpretable and distinct topics, making it advantageous for effective topic selection. Its output includes clear topic themes and keyword distributions, simplifying the process of identifying and selecting relevant topics.\n",
        "On the other hand, LSA (Latent Semantic Analysis) generates broader topics that might not align as precisely with the specific themes of the document, potentially complicating the process of topic selection.\n",
        "\n",
        "Meanwhile, LDA2VEC incorporates semantic relationships, allowing for a more nuanced understanding of topics. BERTOPIC, leveraging the context-rich BERT model, excels at capturing intricate topic nuances.\n",
        "\n",
        "Considering the project's emphasis on topic selection, LDA's transparent and focused outputs make it the best choice. Its output allows for a deeper understanding of the document's key themes, aiding in the efficient selection of the most relevant topics. With its simplicity and interpretability, LDA proves to be the most appropriate and effective solution for topic selection among the four models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9L299j502I0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}