{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NahidFathima/NahidF_INFO5731_Fall2023/blob/main/Syed_NahidFathima_Assignment_Four.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Four**\n",
        "\n",
        "In this assignment, you are required to conduct topic modeling, sentiment analysis based on **the dataset you created from assignment three**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Topic Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(30 points). This question is designed to help you develop a feel for the way topic modeling works, the connection to the human meanings of documents. Based on the dataset from assignment three, write a python program to **identify the top 10 topics in the dataset**. Before answering this question, please review the materials in lesson 8, especially the code for LDA, LSA, and BERTopic. The following information should be reported:\n",
        "\n",
        "(1) Features (text representation) used for topic modeling.\n",
        "\n",
        "(2) Top 10 clusters for topic modeling.\n",
        "\n",
        "(3) Summarize and describe the topic for each cluster.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "outputId": "028e2e10-5c58-4184-ff8b-c40ced784953"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in c:\\users\\nahid\\anaconda3\\lib\\site-packages (4.3.0)\n",
            "Requirement already satisfied: pandas in c:\\users\\nahid\\anaconda3\\lib\\site-packages (2.0.3)\n",
            "Requirement already satisfied: nltk in c:\\users\\nahid\\anaconda3\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from gensim) (1.24.3)\n",
            "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from gensim) (1.11.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: FuzzyTM>=0.4.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from gensim) (2.0.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: click in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
            "Requirement already satisfied: joblib in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
            "Requirement already satisfied: tqdm in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: pyfume in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (0.2.25)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
            "Requirement already satisfied: simpful in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.11.0)\n",
            "Requirement already satisfied: fst-pso in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
            "Requirement already satisfied: miniful in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim pandas nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LU4xARhRbL-",
        "outputId": "2791701a-5a12-44a4-e655-78b43f48a43f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bertopic in c:\\users\\nahid\\anaconda3\\lib\\site-packages (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from bertopic) (1.24.3)\n",
            "Requirement already satisfied: hdbscan>=0.8.29 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from bertopic) (0.8.33)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from bertopic) (0.5.5)\n",
            "Requirement already satisfied: pandas>=1.1.5 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from bertopic) (2.0.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from bertopic) (1.3.0)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from bertopic) (4.65.0)\n",
            "Requirement already satisfied: sentence-transformers>=0.4.1 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from bertopic) (2.2.2)\n",
            "Requirement already satisfied: plotly>=4.7.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from bertopic) (5.9.0)\n",
            "Requirement already satisfied: cython<3,>=0.27 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (0.29.36)\n",
            "Requirement already satisfied: scipy>=1.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (1.11.1)\n",
            "Requirement already satisfied: joblib>=1.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2023.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from plotly>=4.7.0->bertopic) (8.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2.post1->bertopic) (2.2.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (4.32.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (2.1.1)\n",
            "Requirement already satisfied: torchvision in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.16.1)\n",
            "Requirement already satisfied: nltk in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.15.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from tqdm>=4.41.1->bertopic) (0.4.6)\n",
            "Requirement already satisfied: numba>=0.51.2 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from umap-learn>=0.5.0->bertopic) (0.57.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from umap-learn>=0.5.0->bertopic) (0.5.11)\n",
            "Requirement already satisfied: filelock in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.9.0)\n",
            "Requirement already satisfied: fsspec in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2023.4.0)\n",
            "Requirement already satisfied: requests in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (23.1)\n",
            "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.40.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.16.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (1.11.1)\n",
            "Requirement already satisfied: networkx in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.1.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2022.7.9)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.13.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.3.2)\n",
            "Requirement already satisfied: click in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (8.0.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (2.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\nahid\\anaconda3\\lib\\site-packages (from sympy->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install bertopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERA_F3UdRbL_",
        "outputId": "5b736193-0bac-4f65-d709-f9f951bcb456"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features (text representation) used for topic modeling:\n",
            "TF-IDF features are used by default in BERTopic.\n",
            "\n",
            "Top 10 clusters for topic modeling:\n",
            "    Topic  Count\n",
            "23      0    300\n",
            "59      1    200\n",
            "0       2    200\n",
            "47      3    185\n",
            "66     -1    156\n",
            "45      4    100\n",
            "55      5    100\n",
            "60      6    100\n",
            "44      7    100\n",
            "88      8    100\n",
            "\n",
            "Summarize and describe the topic for each cluster:\n",
            "\n",
            "Summary for Cluster 1 (Topic 0):\n",
            "24     Welcome Jurassic Park\n",
            "62     Welcome Jurassic park\n",
            "90     Welcome Jurassic Park\n",
            "124    Welcome Jurassic Park\n",
            "162    Welcome Jurassic park\n",
            "Name: clean_text, dtype: object\n",
            "\n",
            "Summary for Cluster 2 (Topic 1):\n",
            "60     \n",
            "89     \n",
            "160    \n",
            "189    \n",
            "260    \n",
            "Name: clean_text, dtype: object\n",
            "\n",
            "Summary for Cluster 3 (Topic 2):\n",
            "0      Jurassic Park Deleted Scene\n",
            "1      Jurassic Park deleted scene\n",
            "100    Jurassic Park Deleted Scene\n",
            "101    Jurassic Park deleted scene\n",
            "200    Jurassic Park Deleted Scene\n",
            "Name: clean_text, dtype: object\n",
            "\n",
            "Summary for Cluster 4 (Topic 3):\n",
            "48     Jurassic World\n",
            "85     jurassic world\n",
            "148    Jurassic World\n",
            "185    jurassic world\n",
            "248    Jurassic World\n",
            "Name: clean_text, dtype: object\n",
            "\n",
            "Summary for Cluster 5 (Topic -1):\n",
            "68     Jurassic Park I realized John Hammond like lot...\n",
            "86             All I could think watching Jurassic World\n",
            "129    TIL getting collectibles Jurassic Park game ge...\n",
            "329    TIL getting collectibles Jurassic Park game ge...\n",
            "368    Jurassic Park I realized John Hammond like lot...\n",
            "Name: clean_text, dtype: object\n",
            "\n",
            "Summary for Cluster 6 (Topic 4):\n",
            "46     Testing animatronics behind scenes Jurassic Pa...\n",
            "146    Testing animatronics behind scenes Jurassic Pa...\n",
            "246    Testing animatronics behind scenes Jurassic Pa...\n",
            "346    Testing animatronics behind scenes Jurassic Pa...\n",
            "446    Testing animatronics behind scenes Jurassic Pa...\n",
            "Name: clean_text, dtype: object\n",
            "\n",
            "Summary for Cluster 7 (Topic 5):\n",
            "56     This hidden grotto Texas looks like scene Jura...\n",
            "156    This hidden grotto Texas looks like scene Jura...\n",
            "256    This hidden grotto Texas looks like scene Jura...\n",
            "356    This hidden grotto Texas looks like scene Jura...\n",
            "456    This hidden grotto Texas looks like scene Jura...\n",
            "Name: clean_text, dtype: object\n",
            "\n",
            "Summary for Cluster 8 (Topic 6):\n",
            "61     Park\n",
            "161    Park\n",
            "261    Park\n",
            "361    Park\n",
            "461    Park\n",
            "Name: clean_text, dtype: object\n",
            "\n",
            "Summary for Cluster 9 (Topic 7):\n",
            "45     Someone replaced Herman Jurassic Park\n",
            "145    Someone replaced Herman Jurassic Park\n",
            "245    Someone replaced Herman Jurassic Park\n",
            "345    Someone replaced Herman Jurassic Park\n",
            "445    Someone replaced Herman Jurassic Park\n",
            "Name: clean_text, dtype: object\n",
            "\n",
            "Summary for Cluster 10 (Topic 8):\n",
            "94     I quit job middle pandemic become jeweler Six ...\n",
            "194    I quit job middle pandemic become jeweler Six ...\n",
            "294    I quit job middle pandemic become jeweler Six ...\n",
            "394    I quit job middle pandemic become jeweler Six ...\n",
            "494    I quit job middle pandemic become jeweler Six ...\n",
            "Name: clean_text, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "import pandas as pd\n",
        "from bertopic import BERTopic\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Loading my dataset\n",
        "df = pd.read_csv('sentimentanalysis_reddit_data.csv')\n",
        "\n",
        "# Preprocessing\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['clean_text'] = df['clean_text'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word.isalpha() and word not in stop_words]))\n",
        "\n",
        "# Features (text representation) which are used for topic modeling\n",
        "documents = df['clean_text'].tolist()\n",
        "\n",
        "# BERTopic Model\n",
        "topic_model = BERTopic()\n",
        "\n",
        "# Fit the model and transform the documents into topics\n",
        "topics, _ = topic_model.fit_transform(documents)\n",
        "\n",
        "# Assign topics to the DataFrame\n",
        "df['topic'] = topics\n",
        "\n",
        "# 1. Features (text representation) used for topic modeling.\n",
        "print(\"Features (text representation) used for topic modeling:\")\n",
        "print(\"TF-IDF features are used by default in BERTopic.\")\n",
        "\n",
        "# 2. Top 10 clusters for topic modeling.\n",
        "print(\"\\nTop 10 clusters for topic modeling:\")\n",
        "top_clusters_info = topic_model.get_topic_freq().head(10)\n",
        "print(top_clusters_info)\n",
        "\n",
        "# 3. Summarize and describe the topic for each cluster.\n",
        "print(\"\\nSummarize and describe the topic for each cluster:\")\n",
        "for i in range(len(top_clusters_info)):\n",
        "    cluster_id = top_clusters_info.iloc[i]['Topic']\n",
        "    cluster_df = df[df['topic'] == cluster_id]\n",
        "\n",
        "    print(f\"\\nSummary for Cluster {i + 1} (Topic {cluster_id}):\")\n",
        "    print(cluster_df['clean_text'].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaRPCP02RbL_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Sentiment analysis also known as opinion mining is a sub field within Natural Language Processing (NLP) that builds machine learning algorithms to classify a text according to the sentimental polarities of opinions it contains, e.g., positive, negative, neutral. The purpose of this question is to develop a machine learning classifier for sentiment analysis. Based on the dataset from assignment three, write a python program to implement a sentiment classifier and evaluate its performance. Notice: **80% data for training and 20% data for testing**.  \n",
        "\n",
        "(1) Features used for sentiment classification and explain why you select these features.\n",
        "\n",
        "(2) Select two of the supervised learning algorithm from scikit-learn library: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning, to build a sentiment classifier respectively. Note: Cross-validation (5-fold or 10-fold) should be conducted. Here is the reference of cross-validation: https://scikit-learn.org/stable/modules/cross_validation.html.\n",
        "\n",
        "(3) Compare the performance over accuracy, precision, recall, and F1 score for the two algorithms you selected. Here is the reference of how to calculate these metrics: https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vATjQNTY8buA",
        "outputId": "9ca3dab3-ecb4-4338-dce4-09437ef5fc4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performance Metrics for Multinomial Naive Bayes:\n",
            "Accuracy: 0.99825\n",
            "Precision: 0.9978243978243977\n",
            "Recall: 0.9969196919691969\n",
            "F1 Score: 0.9973175782417488\n",
            "\n",
            "\n",
            "Performance Metrics for Logistic Regression:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1 Score: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Loading my dataset\n",
        "df = pd.read_csv('sentimentanalysis_reddit_data.csv')\n",
        "\n",
        "# Features used for sentiment classification\n",
        "texts = df['clean_text']  # 'clean_text' is the column in my dataset with the cleaned text\n",
        "labels = df['sentiment']  # 'sentiment' is the column in my dataset with the sentiment labels\n",
        "\n",
        "# Split the data into training 80% and testing 20% sets\n",
        "texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize the text using CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vectorized = vectorizer.fit_transform(texts_train)\n",
        "X_test_vectorized = vectorizer.transform(texts_test)\n",
        "\n",
        "# 1. Using the the above two features for sentiment classification\n",
        "# CountVectorizer is used to convert text into a matrix of token counts. It's a simple and effective representation for text data.\n",
        "\n",
        "# 2. Select two supervised learning algorithms: Multinomial Naive Bayes and Logistic Regression\n",
        "# 3. Apply cross-validation and evaluate performance metrics\n",
        "\n",
        "# Multinomial Naive Bayes\n",
        "nb_classifier = MultinomialNB()\n",
        "\n",
        "# Cross-validation\n",
        "nb_cv_accuracy = cross_val_score(nb_classifier, X_train_vectorized, labels_train, cv=5, scoring='accuracy').mean()\n",
        "nb_cv_precision = cross_val_score(nb_classifier, X_train_vectorized, labels_train, cv=5, scoring='precision_macro').mean()\n",
        "nb_cv_recall = cross_val_score(nb_classifier, X_train_vectorized, labels_train, cv=5, scoring='recall_macro').mean()\n",
        "nb_cv_f1 = cross_val_score(nb_classifier, X_train_vectorized, labels_train, cv=5, scoring='f1_macro').mean()\n",
        "\n",
        "# Logistic Regression\n",
        "lr_classifier = LogisticRegression()\n",
        "\n",
        "# Cross-validation\n",
        "lr_cv_accuracy = cross_val_score(lr_classifier, X_train_vectorized, labels_train, cv=5, scoring='accuracy').mean()\n",
        "lr_cv_precision = cross_val_score(lr_classifier, X_train_vectorized, labels_train, cv=5, scoring='precision_macro').mean()\n",
        "lr_cv_recall = cross_val_score(lr_classifier, X_train_vectorized, labels_train, cv=5, scoring='recall_macro').mean()\n",
        "lr_cv_f1 = cross_val_score(lr_classifier, X_train_vectorized, labels_train, cv=5, scoring='f1_macro').mean()\n",
        "\n",
        "# Print the performance metrics\n",
        "print(\"Performance Metrics for Multinomial Naive Bayes:\")\n",
        "print(f\"Accuracy: {nb_cv_accuracy}\")\n",
        "print(f\"Precision: {nb_cv_precision}\")\n",
        "print(f\"Recall: {nb_cv_recall}\")\n",
        "print(f\"F1 Score: {nb_cv_f1}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Performance Metrics for Logistic Regression:\")\n",
        "print(f\"Accuracy: {lr_cv_accuracy}\")\n",
        "print(f\"Precision: {lr_cv_precision}\")\n",
        "print(f\"Recall: {lr_cv_recall}\")\n",
        "print(f\"F1 Score: {lr_cv_f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3: House price prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(20 points). You are required to build a **regression** model to predict the house price with 79 explanatory variables describing (almost) every aspect of residential homes. The purpose of this question is to practice regression analysis, an supervised learning model. The training data, testing data, and data description files can be download from canvas. Here is an axample for implementation: https://towardsdatascience.com/linear-regression-in-python-predict-the-bay-areas-home-price-5c91c8378878.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfvMKJjIXS5G",
        "outputId": "2fc05ebd-c7d3-4893-deb0-7aa5e96e30c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Root Mean Squared Error (RMSE): 29477.1519862372\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Loading the training dataset\n",
        "train_path = \"C:\\\\Users\\\\Nahid\\\\Downloads\\\\train.csv\"\n",
        "train_data = pd.read_csv(train_path)\n",
        "\n",
        "# Loading the testing dataset\n",
        "test_path = \"C:\\\\Users\\\\Nahid\\\\Downloads\\\\test.csv\"\n",
        "test_data = pd.read_csv(test_path)\n",
        "\n",
        "# Identify features (X) and target variable (y) in the training data\n",
        "X_train = train_data.drop('SalePrice', axis=1)\n",
        "y_train = train_data['SalePrice']\n",
        "\n",
        "# Define numerical and categorical features\n",
        "numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = X_train.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Create transformers for numerical and categorical features\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Combine transformers using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Create the final pipeline with the model\n",
        "model = LinearRegression()\n",
        "\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                             ('model', model)])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "predictions = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "rmse = mse**0.5\n",
        "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
        "\n",
        "# Now, use the trained model to predict house prices on the testing data\n",
        "test_predictions = pipeline.predict(test_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAbuuu7jRbMA",
        "outputId": "79bdb244-5067-449b-bb80-71312fe11291"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-Validation RMSE Scores: [26829.02988336 31781.67684855 31536.32759815 24698.39039142\n",
            " 35263.77238848]\n",
            "Mean RMSE: 30021.83942199284\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "# Loading the training dataset\n",
        "train_path = \"C:\\\\Users\\\\Nahid\\\\Downloads\\\\train.csv\"\n",
        "train_data = pd.read_csv(train_path)\n",
        "\n",
        "# Identify features (X) and target variable (y) in the training data\n",
        "X_train = train_data.drop('SalePrice', axis=1)\n",
        "y_train = train_data['SalePrice']\n",
        "\n",
        "# Identify categorical and numerical features\n",
        "categorical_features = X_train.select_dtypes(include=['object']).columns\n",
        "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Create a preprocessing pipeline\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Define the pipeline with preprocessing steps and the model\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', RandomForestRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "# Cross-validation\n",
        "cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
        "rmse_cv_scores = (-cv_scores)**0.5\n",
        "\n",
        "print(\"Cross-Validation RMSE Scores:\", rmse_cv_scores)\n",
        "print(\"Mean RMSE:\", rmse_cv_scores.mean())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BbswDvnEX-k"
      },
      "source": [
        "# **Question 4: Using Pre-trained LLMs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKwKTnW1EX-k"
      },
      "source": [
        "(20 points)\n",
        "Utilize a **pre-trained Large Language Model (LLM) from the Hugging Face Repository** for your specific task using the data collected in Assignment 3. After creating an account on Hugging Face (https://huggingface.co/), choose a relevant LLM from their repository, such as GPT-3, BERT, or RoBERTa or any Meta based text analysis model. Provide a brief description of the selected LLM, including its original sources, significant parameters, and any task-specific fine-tuning if applied.\n",
        "\n",
        "Perform a detailed analysis of the LLM's performance on your task, including key metrics, strengths, and limitations. Additionally, discuss any challenges encountered during the implementation and potential strategies for improvement. This will enable a comprehensive understanding of the chosen LLM's applicability and effectiveness for the given task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "6bd8bede561e4fd69563e48d37441465",
            "1b22ab7c29eb4b59bf793e79f007e74c",
            "1d26f3243ca844159a5cdae7a2e0c79f",
            "ddedd42c800e462884cd201e8e3e3398",
            "70c8cdaf6bcf4daeb722c9f9e690dae1"
          ]
        },
        "id": "0rvjcaStRbMB",
        "outputId": "75169a98-e852-4628-9905-0d6370cc2e3a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6bd8bede561e4fd69563e48d37441465",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b22ab7c29eb4b59bf793e79f007e74c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d26f3243ca844159a5cdae7a2e0c79f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ddedd42c800e462884cd201e8e3e3398",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70c8cdaf6bcf4daeb722c9f9e690dae1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                             clean_text sentiment  emotion  \\\n",
            "0                           Jurassic Park Deleted Scene   neutral  neutral   \n",
            "1                           Jurassic Park deleted scene   neutral  neutral   \n",
            "2     So, I've removed some animations from Jurassic...   neutral  neutral   \n",
            "3     During the filming of Jurassic Park (1993), T-...   neutral  neutral   \n",
            "4     My daughter watching Jurassic Bark for the fir...   neutral  neutral   \n",
            "...                                                 ...       ...      ...   \n",
            "9995  People would probably still visit Jurassic Par...  positive      joy   \n",
            "9996  In Jurassic Park, you can see Dr. Wu erasing a...  negative  sadness   \n",
            "9997                       Jurassic Parks & Recreation.   neutral  neutral   \n",
            "9998                   Shitty New Jurassic World Poster  negative  sadness   \n",
            "9999          New Poster for 'Jurassic World: Dominion'   neutral  neutral   \n",
            "\n",
            "     predicted_emotion  \n",
            "0              LABEL_1  \n",
            "1              LABEL_1  \n",
            "2              LABEL_1  \n",
            "3              LABEL_1  \n",
            "4              LABEL_1  \n",
            "...                ...  \n",
            "9995           LABEL_1  \n",
            "9996           LABEL_1  \n",
            "9997           LABEL_1  \n",
            "9998           LABEL_1  \n",
            "9999           LABEL_1  \n",
            "\n",
            "[10000 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "# Loading my dataset\n",
        "\n",
        "df = pd.read_csv('sentimentanalysis_reddit_data.csv')\n",
        "\n",
        "# Check for any missing values\n",
        "df = df.dropna(subset=['clean_text', 'sentiment'])\n",
        "\n",
        "# Define the emotion detection pipeline using distilroberta-base\n",
        "emotion_pipeline = pipeline(\"sentiment-analysis\", model=\"distilroberta-base\")\n",
        "\n",
        "# Function to map sentiment labels to emotions\n",
        "def map_sentiment_to_emotion(sentiment_label):\n",
        "\n",
        "    # Map positive sentiment to joy, negative sentiment to sadness, and neutral to neutral\n",
        "    if sentiment_label == 'positive':\n",
        "        return 'joy'\n",
        "    elif sentiment_label == 'negative':\n",
        "        return 'sadness'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "# Apply emotion detection to each clean_text\n",
        "df['emotion'] = df['sentiment'].apply(lambda x: map_sentiment_to_emotion(x.lower()))\n",
        "df['predicted_emotion'] = df['clean_text'].apply(lambda x: emotion_pipeline(x)[0]['label'])\n",
        "\n",
        "# Display the results\n",
        "print(df[['clean_text', 'sentiment', 'emotion', 'predicted_emotion']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJfTf9y9RbMB",
        "outputId": "d383ce59-0e9c-49d5-f884-5578a56a18a7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                             clean_text sentiment  emotion  \\\n",
            "0                           Jurassic Park Deleted Scene   neutral  neutral   \n",
            "1                           Jurassic Park deleted scene   neutral  neutral   \n",
            "2     So, I've removed some animations from Jurassic...   neutral  neutral   \n",
            "3     During the filming of Jurassic Park (1993), T-...   neutral  neutral   \n",
            "4     My daughter watching Jurassic Bark for the fir...   neutral  neutral   \n",
            "...                                                 ...       ...      ...   \n",
            "9995  People would probably still visit Jurassic Par...  positive      joy   \n",
            "9996  In Jurassic Park, you can see Dr. Wu erasing a...  negative  sadness   \n",
            "9997                       Jurassic Parks & Recreation.   neutral  neutral   \n",
            "9998                   Shitty New Jurassic World Poster  negative  sadness   \n",
            "9999          New Poster for 'Jurassic World: Dominion'   neutral  neutral   \n",
            "\n",
            "     predicted_emotion  \n",
            "0              LABEL_1  \n",
            "1              LABEL_1  \n",
            "2              LABEL_1  \n",
            "3              LABEL_1  \n",
            "4              LABEL_1  \n",
            "...                ...  \n",
            "9995           LABEL_1  \n",
            "9996           LABEL_1  \n",
            "9997           LABEL_1  \n",
            "9998           LABEL_1  \n",
            "9999           LABEL_1  \n",
            "\n",
            "[10000 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "# Loading my dataset\n",
        "df = pd.read_csv('sentimentanalysis_reddit_data.csv')\n",
        "\n",
        "# Check for any missing values\n",
        "df = df.dropna(subset=['clean_text', 'sentiment'])\n",
        "\n",
        "# Define the emotion detection pipeline using distilroberta-base\n",
        "emotion_pipeline = pipeline(\"sentiment-analysis\", model=\"distilroberta-base\")\n",
        "\n",
        "# Function to map sentiment labels to emotions\n",
        "def map_sentiment_to_emotion(sentiment_label):\n",
        "\n",
        "    #Map positive sentiment to joy, negative sentiment to sadness, and neutral to neutral\n",
        "    if sentiment_label == 'positive':\n",
        "        return 'joy'\n",
        "    elif sentiment_label == 'negative':\n",
        "        return 'sadness'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "# Apply emotion detection to each clean_text\n",
        "df['emotion'] = df['sentiment'].apply(lambda x: map_sentiment_to_emotion(x.lower()))\n",
        "df['predicted_emotion'] = df['clean_text'].apply(lambda x: emotion_pipeline(x)[0]['label'])\n",
        "\n",
        "# Display the results\n",
        "print(df[['clean_text', 'sentiment', 'emotion', 'predicted_emotion']])\n",
        "\n",
        "# Save the results to a new CSV file\n",
        "df.to_csv('sentiment_analysis_results.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzJihU3JRbMB",
        "outputId": "24fe1dae-8882-4822-c057-385c810d674b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentiment Label Distribution:\n",
            "sentiment\n",
            "neutral     5500\n",
            "positive    2600\n",
            "negative    1900\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Predicted Emotion Label Distribution:\n",
            "predicted_emotion\n",
            "LABEL_1    10000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Checking the distribution of sentiment labels\n",
        "print(\"Sentiment Label Distribution:\")\n",
        "print(df['sentiment'].value_counts())\n",
        "\n",
        "# Checking the distribution of predicted emotion labels\n",
        "print(\"\\nPredicted Emotion Label Distribution:\")\n",
        "print(df['predicted_emotion'].value_counts())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKTQCK7qRbMB"
      },
      "source": [
        "Brief Description: The distilroberta-base model is part of the Hugging Face Transformers library and is based on the RoBERTa architecture. RoBERTa (Robustly optimized BERT approach) is a modification of BERT (Bidirectional Encoder Representations from Transformers) introduced by Facebook AI in a research paper. Distilroberta is a distilled version of RoBERTa, meaning it's a smaller and faster variant while retaining much of the original model's performance.\n",
        "\n",
        "Significant Parameters: The distilroberta-base model consists of 82 million parameters. It has 6 layers, 768 hidden units per layer, and 12 attention heads.\n",
        "\n",
        "Strengths: The distilroberta-base model is computationally less intensive than larger models like RoBERTa, making it faster and more memory-efficient.\n",
        "It inherits the strong performance of RoBERTa on various natural language understanding tasks.\n",
        "\n",
        "Limitations: Smaller models like distilroberta may not capture as much context and nuance as larger models.\n",
        "The model's performance heavily depends on the quality and representativeness of the training data.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}